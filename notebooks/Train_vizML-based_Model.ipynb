{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook loads data and trains a neural network using data from the Plotly community feed.  \n",
    "\n",
    "It makes heavy use of the modified vizML code, and that code can also be run independently using the agg.py script: from within the `neural_network` directory, use `python agg.py` followed by `LOAD` then `TRAIN` then `EVAL`.\n",
    "\n",
    "This notebook requires specific versions of several packages-- create a virtual environment, then `pip install -r requirements.txt`\n",
    "\n",
    "It also requires the extracted features from the plotly community feed.  If training a general model, you can download the prepaired features from vizML using the retrieve_vizML_data.sh script. If training on custom data, first run data cleaning and feature extraction scripts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "#import neural_network.evaluate as evaluate\n",
    "import neural_network.util as util\n",
    "import neural_network.nets as nets\n",
    "import neural_network.train as train\n",
    "from helpers.processing import *\n",
    "from helpers.analysis import *\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "features_directory = '../features/processed'\n",
    "saves_directory = '../neural_network/saves'\n",
    "num_datapoints = None  # None if you want all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script tests aggregate features over line, scatter, and bar outcomes\n",
    "def load_features():\n",
    "    features_df_file_name = 'features_aggregate_single.csv'\n",
    "    outcomes_df_file_name = 'chart_outcomes.csv'\n",
    "\n",
    "    features_df = pd.read_csv(\n",
    "        os.path.join(\n",
    "            features_directory,\n",
    "            features_df_file_name))\n",
    "    outcomes_df = pd.read_csv(\n",
    "        os.path.join(\n",
    "            features_directory,\n",
    "            outcomes_df_file_name))\n",
    "\n",
    "    features_df = features_df[:num_datapoints]\n",
    "    outcome_variable_name = 'all_one_trace_type'\n",
    "    # photon: add circos here? \n",
    "    outcomes = ['line', 'scatter', 'bar']\n",
    "    outcomes_df_subset = outcomes_df[outcomes_df[outcome_variable_name].isin(\n",
    "        outcomes)][['fid', outcome_variable_name]]\n",
    "\n",
    "    final_df = pd.merge(features_df, outcomes_df_subset, on='fid', how='inner')\n",
    "    final_df = final_df.drop(['fid'], axis=1, inplace=False, errors='ignore')\n",
    "    final_df.sample(frac=1.0)\n",
    "\n",
    "    last_index = final_df.columns.get_loc(outcome_variable_name)\n",
    "    X = final_df.iloc[:, :last_index]\n",
    "    y = final_df.iloc[:, last_index]\n",
    "    y = pd.get_dummies(y).values.argmax(1)\n",
    "\n",
    "    res = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "    X, y = res.fit_sample(X, y)\n",
    "    # shuffle X and y in unison, and then return\n",
    "    return util.unison_shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the parameters for the script can be adjusted here\n",
    "\n",
    "# num_epochs: the max number of epochs we will train the NN for\n",
    "# hidden_sizes: the number of neurons in each hidden layer, enter it as a list\n",
    "# output_dim: the dimension of the output. Since outputs are 'line', 'scatter', 'bar', it's 3\n",
    "#                                                            + 'circos' ; +  'table' will be 5\n",
    "# weight_decay: how much to decay LR in the NN. This can be set to 0 since we decrease LR already through\n",
    "#   the ReduceLROnPlateau() function\n",
    "# dropout: the dropout in each layer\n",
    "# patience: how many epochs we go through (with a near constant learning rate, this threshold is adjusted using\n",
    "#   threshold) before dropping learning rate by a factor of 10\n",
    "# model_prefix: all models will be loaded/saved with the prefix of the file in the beginning\n",
    "# save_model: save each epoch's model onto models/ folder.\n",
    "# print_test: print test accuracies into test.txt\n",
    "# test_best: test the test accuracy of the best model we've found (best\n",
    "# model determined using val accuracy)\n",
    "\n",
    "# note: training is automatically stopped when learning rate < 0.01 *\n",
    "# starting learning rate\n",
    "\n",
    "parameters = {\n",
    "    'batch_size': 200,\n",
    "    'num_epochs': 100,\n",
    "    'hidden_sizes': [800, 800, 800],\n",
    "    'learning_rate': 5e-4,\n",
    "    'output_dim': 3,\n",
    "    'weight_decay': 0,\n",
    "    'dropout': 0.00,\n",
    "    'patience': 20,\n",
    "    'threshold': 1e-3,\n",
    "    'model_prefix': 'agg',\n",
    "    'save_model': False,\n",
    "    'print_test': True,\n",
    "    'test_best': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of total examples is  210\n",
      "indexes for splitting between train/val/test are  [168, 189]\n"
     ]
    }
   ],
   "source": [
    "X, y = load_features()\n",
    "# split 10% of examples into val, and 10% into test\n",
    "util.save_matrices_to_disk(\n",
    "    X, y, [0.1, 0.1], saves_directory, parameters['model_prefix'], num_datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = util.load_matrices_from_disk(\n",
    "        saves_directory, parameters['model_prefix'], num_datapoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# set \"device\" to regular cpu if no GPU is available \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# to-do: maybe add this to the train.py script? \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dim is 3\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = train.load_datasets(\n",
    "    X_train.astype(np.float64), y_train.astype(np.float64), X_val.astype(np.float64), y_val.astype(np.float64), parameters, X_test=X_test.astype(np.float64), y_test=y_test.astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# batch_size is determined in the dataloader, so the variable is irrelevant here\n",
    "batch_size = parameters.get('batch_size', 200)\n",
    "num_epochs = parameters.get('num_epochs', 100)\n",
    "hidden_sizes = parameters.get('hidden_sizes', [200])\n",
    "learning_rate = parameters.get('learning_rate', 0.0005)\n",
    "weight_decay = parameters.get('weight_decay', 0)\n",
    "dropout = parameters.get('dropout', 0.0)\n",
    "patience = parameters.get('patience', 10)\n",
    "threshold = parameters.get('threshold', 1e-3)\n",
    "input_dim = parameters['input_dim']\n",
    "output_dim = parameters['output_dim']\n",
    "# output_period: output training loss every x batches\n",
    "output_period = parameters.get('output_period', 0)\n",
    "model_prefix = parameters.get('model_prefix', None)\n",
    "only_train = parameters.get('only_train', False)\n",
    "save_model = parameters.get('save_model', False)\n",
    "test_best = parameters.get('test_best', False)\n",
    "print_test = parameters.get(\n",
    "    'print_test', False) and (\n",
    "    test_dataloader is not None)\n",
    "# set device to cuda if available, cpu if not\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device is \", device)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "657"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out test accuracies to a separate file\n",
    "suffix=''\n",
    "test_file = None\n",
    "if print_test:\n",
    "    test_file = open('test{}.txt'.format(suffix), 'a')\n",
    "    test_file.write('\\n\\n')\n",
    "    test_file.write('Starting at ' + util.get_time() + '\\n')\n",
    "    test_file.write(', '.join(['{}={!r}'.format(k, v) for k, v in sorted(parameters.items())]) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a model that can be passed the above parameters\n",
    "class AdvancedNet(nn.Module):\n",
    "    '''\n",
    "    I'm borrowing AdvancedNet from vizML. \n",
    "    It uses nn.ModuleList to construct a list of modules/layers according to the above paramiters. \n",
    "    hidden_sizes is a list where each intiger becomes the number of neurons in a hidden layer. \n",
    "    By default it is [800, 800, 800] so makes a network with 3 hidden layers.\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes, dropout=0.0):\n",
    "        super(AdvancedNet, self).__init__()\n",
    "        self.nn_list = nn.ModuleList()\n",
    "        self.nn_list.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        self.nn_list.append(nn.ReLU())\n",
    "        if dropout:\n",
    "            self.nn_list.append(nn.Dropout(p=dropout))\n",
    "\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            self.nn_list.append(\n",
    "                nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "            self.nn_list.append(nn.ReLU())\n",
    "            if dropout:\n",
    "                self.nn_list.append(nn.Dropout(p=dropout))\n",
    "        self.nn_list.append(nn.Linear(hidden_sizes[-1], num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self.nn_list:\n",
    "            x = module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_classes, dropout=0.0):\n",
    "            super(Feedforward, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "            self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "            self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "            self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "            self.fc5 = nn.Linear(hidden_size, num_classes)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = nn.functional.relu(self.fc1(x))\n",
    "            x = nn.functional.relu(self.fc2(x))\n",
    "            x = nn.functional.relu(self.fc3(x))\n",
    "            x = nn.functional.relu(self.fc4(x))\n",
    "            x = self.fc5(x)\n",
    "            return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "    '''\n",
    "    This is my own model, more explicitly written\n",
    "    Less flexible than AdvancedNet becayse you can't change the \n",
    "    architecture with a change to hidden_sizes -- but identical otherwise.\n",
    "    Useful for debugging the cuda stuff.\n",
    "    Change models below \n",
    "    '''\n",
    "        def __init__(self, input_size, hidden_size, num_classes, dropout=0.0):\n",
    "            super(Feedforward, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.fc2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "            self.fc3 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "            self.fc4 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "            self.fc5 = nn.Linear(self.hidden_size, self.num_classes)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.relu(self.fc3(x))\n",
    "            x = F.relu(self.fc4(x))\n",
    "            x = self.fc5(x)\n",
    "            return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set key paramiters\n",
    "input_dim = parameters['input_dim']\n",
    "output_dim = parameters['output_dim']\\dropout = parameters.get('dropout', 0.0)\n",
    "learning_rate = parameters.get('learning_rate', 0.0005)\n",
    "weight_decay = parameters.get('weight_decay', 0)\n",
    "# hidden_size (a value) is different than hidden_sizes (a list)\n",
    "# Feedforward model applies hidden_size to all hidden layers\n",
    "# while AdvancedNet applies the list hidden_sizes itteratively to create layesr\n",
    "hidden_size = 800\n",
    "# nets and optimizers\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "model = Feedforward(\n",
    "    input_dim,\n",
    "    hidden_size,\n",
    "    output_dim,\n",
    "    dropout=dropout).to(device)\n",
    "optimizer = optim.Adam(\n",
    "    net.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay)\n",
    "\n",
    "# ReduceLROnPlateau reduces learning rate by factor of 10 once val loss has plateaued\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=patience, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()                                                  # Set model parameters to training mode\n",
    "    running_loss = 0.0                                             # Keep track of running loss so average loss can be calculated at the end of the epoch\n",
    "    all_true = []                                                  # Keep track of all of the true and predicted labels\n",
    "    all_pred = []                                                  #   so AUC score can be calculated at the end of the epoch\n",
    "    \n",
    "    for batch_num, (inputs, labels) in enumerate(dataloader):      # Loop through all batches in the dataloader\n",
    "        \n",
    "        all_true += labels.tolist()                                # Save true labels\n",
    "        inputs = inputs.to(device)                                 # Send images and labels to computing device. If the model is on 'cuda',\n",
    "        labels = labels.to(device)                                 #   the images and labels must also be on cuda\n",
    "        \n",
    "        optimizer.zero_grad()                                      # Zero the gradients so the optimizer can keep track of a new pass of data through the network\n",
    "        pred = model(inputs)                                         # Pass a batch of images through the model\n",
    "        loss = criterion(pred, labels)                             # Calculate the loss between the ground truth labels and the model predictions\n",
    "        loss.backward()                                            # Perform backpropagation on the loss to train the network\n",
    "        optimizer.step()                                           # Step the optimizer forward\n",
    "        \n",
    "        all_pred += torch.sigmoid(pred).tolist()                   # Keep track of model predictions to calculate AUC later.\n",
    "                                                                   #   When we want to make predictions with our model without calculating BCEWithLogitsLoss,\n",
    "                                                                   #   we must manually apply the sigmoid function to the model output to get predicted values\n",
    "                                                                   #   between 0 and 1. tolist() simply converts the result to a normal Python list.\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    return running_loss / (i+1), roc_auc_score(all_true, all_pred, average='weighted')    # Return the average loss over the epoch and the AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, criterion, device):\n",
    "    model.eval()                                                   # Set model parameters to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        all_true += labels.tolist()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():                                      # Don't track gradients through the model\n",
    "            pred = model(inputs)\n",
    "            loss = criterion(pred, labels)\n",
    "            all_pred += torch.sigmoid(pred).tolist()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    return running_loss / (i+1), roc_auc_score(all_true, all_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_dataloader, val_dataloader, optimizer, criterion, device, num_epochs):\n",
    "    # Log start time\n",
    "    start_time = str(datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S'))\n",
    "    msg = f'Start model training {start_time}'\n",
    "    with open('log.txt', 'a') as f: f.write(msg+'\\n')\n",
    "    print(msg)\n",
    "    \n",
    "    # Initialize best model weights, AUC score, and keep track of train/val loss and AUC\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_model_name = ''\n",
    "    best_score = -1\n",
    "    track_values = {'train_loss': [],\n",
    "                    'val_loss': [],\n",
    "                    'train_auc': [],\n",
    "                    'val_auc': []}\n",
    "\n",
    "    # Calculate initial loss and score on train and validation sets\n",
    "    start = time.time()\n",
    "    train_loss, train_score = test(model, train_dataloader, criterion, device)\n",
    "    val_loss, val_score = test(model, val_dataloader, criterion, device)\n",
    "    \n",
    "    # Store initial losses and AUC scores\n",
    "    track_values['train_loss'].append(train_loss)\n",
    "    track_values['val_loss'].append(val_loss)\n",
    "    track_values['train_auc'].append(train_score)\n",
    "    track_values['val_auc'].append(val_score)\n",
    "    \n",
    "    # Print training status and write to a log file\n",
    "    msg = f'Epoch 0/{num_epochs} Train Loss: {train_loss:.4f}, Train AUC: {train_score:.4f}, Val Loss: {val_loss:.4f}, Val AUC: {val_score:.4f} Time: {time.time()-start:.2f}s'\n",
    "    with open('log.txt', 'a') as f: f.write(msg+'\\n')\n",
    "    print(msg)\n",
    "\n",
    "    # Loop over the specified number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time() # Start timer to keep track of how long an epoch takes\n",
    "\n",
    "        # Run train and test functions on train and val sets\n",
    "        train_loss, train_score = train(model, train_dataloader, optimizer, criterion, device)\n",
    "        val_loss, val_score = test(model, val_dataloader, criterion, device)\n",
    "        \n",
    "        # Store losses and AUC scores\n",
    "        track_values['train_loss'].append(train_loss)\n",
    "        track_values['val_loss'].append(val_loss)\n",
    "        track_values['train_auc'].append(train_score)\n",
    "        track_values['val_auc'].append(val_score)\n",
    "\n",
    "        # Save the model weights if the AUC score on the validation set is higher than the previous best model\n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if os.path.exists(best_model_name): os.remove(best_model_name)\n",
    "            best_model_name = f'./best_model_weights_epoch_{epoch+1}_auc_{val_score:.4f}.pt'\n",
    "            torch.save(model.state_dict(), best_model_name)\n",
    "\n",
    "        # Print training status and write to a log file\n",
    "        msg = f'Epoch {epoch+1}/{num_epochs} Train Loss: {train_loss:.4f}, Train AUC: {train_score:.4f}, Val Loss: {val_loss:.4f}, Val AUC: {val_score:.4f} Time: {time.time()-start:.2f}s'\n",
    "        with open('log.txt', 'a') as f: f.write(msg+'\\n')\n",
    "        print(msg)\n",
    "    \n",
    "    return track_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model training 2020-06-19-13-44-22\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Float but got scalar type Double for argument #2 'mat1' in call to _th_addmm",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-b885aa2413ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# start training!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrack_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-182-1cd74f29f10d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, criterion, device, num_epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Calculate initial loss and score on train and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-181-18bf07435132>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, dataloader, criterion, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                                      \u001b[0;31m# Don't track gradients through the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mall_pred\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-175-d3c8d1f7732e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Float but got scalar type Double for argument #2 'mat1' in call to _th_addmm"
     ]
    }
   ],
   "source": [
    "# start training! \n",
    "track_values = fit(model, train_dataloader, val_dataloader, optimizer, criterion, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at 2020-06-19 06:49:45\n",
      "batch_size=200, dropout=0.0, hidden_sizes=[800, 800, 800], input_dim=657, learning_rate=0.0005, model_prefix='agg', num_epochs=100, output_dim=3, patience=20, print_test=True, save_model=False, test_best=False, threshold=0.001, weight_decay=0\n",
      "starting training\n",
      "epoch: 1, lr: 5.0e-04    2020-06-19 06:49:45\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-4ddc953d3494>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m### here's where it's hanging up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-a837cb771ece>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "# start trainging \n",
    "num_train_batches = len(train_dataloader)\n",
    "epoch = 1\n",
    "best_epoch, best_acc = 0, 0\n",
    "train_acc = [0]\n",
    "\n",
    "print('Starting training at ' + util.get_time())\n",
    "print(', '.join(['{}={!r}'.format(k, v)\n",
    "                for k, v in sorted(parameters.items())]))\n",
    "\n",
    "# print out test accuracies to a separate file\n",
    "suffix=''\n",
    "test_file = None\n",
    "if print_test:\n",
    "    test_file = open('test{}.txt'.format(suffix), 'a')\n",
    "    test_file.write('\\n\\n')\n",
    "    test_file.write('Starting at ' + util.get_time() + '\\n')\n",
    "    test_file.write(', '.join(['{}={!r}'.format(k, v) for k, v in sorted(parameters.items())]) + '\\n\\n')\n",
    "\n",
    "print('starting training')\n",
    "while epoch <= num_epochs:\n",
    "    running_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "\n",
    "    net.train()\n",
    "    print(\n",
    "        'epoch: %d, lr: %.1e' %\n",
    "        (epoch,\n",
    "        optimizer.param_groups[0]['lr']) +\n",
    "        '    ' +\n",
    "        util.get_time())\n",
    "    for batch_num, (inputs, labels) in enumerate(train_dataloader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = net(inputs) ### here's where it's hanging up \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # output is 2D array (logsoftmax output), so we flatten it to a 1D to get the max index for each example\n",
    "        # and then calculate accuracy off that\n",
    "        max_index = outputs.max(dim=1)[1]\n",
    "        epoch_acc += np.sum(max_index.data.cpu().numpy()\n",
    "                           == labels.data.cpu().numpy()) / inputs.size()[0]\n",
    "\n",
    "        # output every output_period batches\n",
    "        if output_period:\n",
    "            if batch_num % output_period == 0:\n",
    "                print('[%d:%.2f] loss: %.3f' % (\n",
    "                    epoch, batch_num * 1.0 / num_train_batches,\n",
    "                    running_loss / output_period))\n",
    "                running_loss = 0.0\n",
    "                gc.collect()\n",
    "\n",
    "    # save model after every epoch in models/ folder\n",
    "    if save_model:\n",
    "        torch.save(\n",
    "            net.state_dict(),\n",
    "            models_directory +\n",
    "            '/' +\n",
    "            model_prefix +\n",
    "            \".%d\" %\n",
    "            epoch)\n",
    "\n",
    "    # print training/val accuracy\n",
    "    epoch_acc = epoch_acc / num_train_batches\n",
    "    train_acc.append(epoch_acc)\n",
    "    print('train acc: %.4f' % (epoch_acc))\n",
    "    if only_train:\n",
    "        scheduler.step(loss)\n",
    "    else:\n",
    "        val_accuracy, total_loss = evaluate.eval_error(\n",
    "            net, val_dataloader, criterion)\n",
    "        print('val acc: %.4f, loss: %.4f' % (val_accuracy, total_loss))\n",
    "        # remember: feed val loss into scheduler\n",
    "        scheduler.step(total_loss)\n",
    "        if val_accuracy > best_acc:\n",
    "            best_epoch, best_acc = epoch, val_accuracy\n",
    "        print()\n",
    "\n",
    "        # write test accuracy\n",
    "        if print_test:\n",
    "            test_accuracy, total_loss = evaluate.eval_error(\n",
    "                net, test_dataloader, criterion)\n",
    "            test_file.write(\n",
    "                'epoch: %d' %\n",
    "                (epoch) +\n",
    "                '    ' +\n",
    "                util.get_time() +\n",
    "                '\\n')\n",
    "            test_file.write('train acc: %.4f' % (epoch_acc) + '\\n')\n",
    "            test_file.write('val acc: %.4f' % (val_accuracy) + '\\n')\n",
    "            test_file.write('test acc: %.4f' % (test_accuracy) + '\\n')\n",
    "            test_file.write('loss: %.4f' % (test_accuracy) + '\\n')\n",
    "\n",
    "        gc.collect()\n",
    "            \n",
    "    # perform early stopping here if our learning rate is below a threshold\n",
    "    # because small lr means little change in accuracy anyways\n",
    "    if optimizer.param_groups[0]['lr'] < (0.9 * 0.01 * learning_rate):\n",
    "        print('Low LR reached, finishing training early')\n",
    "        break\n",
    "    epoch += 1\n",
    "\n",
    "print('best epoch: %d' % best_epoch)\n",
    "print('best val accuracy: %.4f' % best_acc)\n",
    "print('train accuracy at that epoch: %.4f' % train_acc[best_epoch])\n",
    "print('ending at', time.ctime())\n",
    "\n",
    "if test_best:\n",
    "    net.load_state_dict(\n",
    "        torch.load(\n",
    "            models_directory +\n",
    "            '/' +\n",
    "            model_prefix +\n",
    "            '.' +\n",
    "            str(best_epoch)))\n",
    "    best_test_accuracy, total_loss = evaluate.eval_error(\n",
    "        net, test_dataloader, criterion)\n",
    "    test_file.write('*****\\n')\n",
    "    test_file.write(\n",
    "        'best test acc: %.4f, loss: %.4f' %\n",
    "        (best_test_accuracy, total_loss) + '\\n')\n",
    "    test_file.write('*****\\n')\n",
    "    print('best test acc: %.4f, loss: %.4f' %\n",
    "        (best_test_accuracy, total_loss))\n",
    "\n",
    "if print_test:\n",
    "    test_file.write('\\n')\n",
    "    test_file.close()\n",
    "\n",
    "print('\\n\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-1.4-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
