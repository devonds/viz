{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Chart Type with a Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "base_path = os.path.abspath(os.path.join('..'))\n",
    "if base_path not in sys.path:\n",
    "    sys.path.append(base_path)\n",
    "    \n",
    "import neural_network.util as util\n",
    "import neural_network.train as train\n",
    "import neural_network.evaluate as evaluate\n",
    "from helpers.processing import *\n",
    "from helpers.analysis import *\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "features_directory = '../features/processed'\n",
    "saves_directory = '../neural_network/saves'\n",
    "num_datapoints = None  # None if you want all of the data\n",
    "model_prefix = 'agg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the parameters can be adjusted here\n",
    "\n",
    "# num_epochs: the max number of epochs we will train the NN for\n",
    "# hidden_sizes: the number of neurons in each hidden layer, enter it as a list\n",
    "# output_dim: the dimension of the output. Since outputs are 'line', 'scatter', 'bar', it's 9\n",
    "#                                                            + 'circos' ; +  'table' will be 5\n",
    "# weight_decay: how much to decay LR in the NN. This can be set to 0 since we decrease LR already through\n",
    "#   the ReduceLROnPlateau() function\n",
    "# dropout: the dropout in each layer\n",
    "# patience: how many epochs we go through (with a near constant learning rate, this threshold is adjusted using\n",
    "#   threshold) before dropping learning rate by a factor of 10\n",
    "# model_prefix: all models will be loaded/saved with the prefix of the file in the beginning\n",
    "# save_model: save each epoch's model onto models/ folder.\n",
    "# print_test: print test accuracies into test.txt\n",
    "# test_best: test the test accuracy of the best model we've found (best model determined using val accuracy)\n",
    "\n",
    "# note: training is automatically stopped when learning rate < 0.01 *\n",
    "# batch_size is determined in the dataloader, so the variable is irrelevant here\n",
    "batch_size = 200\n",
    "num_epochs = 100\n",
    "hidden_sizes = [800, 800, 800] # AdvancedNet (from vizML) uses a list of hidden size values\n",
    "hidden_size = 800 # FeedForward model uses a single hidden size value\n",
    "learning_rate = 5e-4\n",
    "weight_decay = 0\n",
    "dropout = 0.0\n",
    "patience = 10\n",
    "threshold = 1e-3\n",
    "input_dim = X_train.shape[1]\n",
    "input_size = X_train.shape[1]\n",
    "output_dim = len(chart_names)\n",
    "output_period = 0 # output_period: output training loss every x batches\n",
    "model_prefix = 'agg'\n",
    "only_train = False\n",
    "save_model = False\n",
    "test_best = False\n",
    "print_test = True\n",
    "\n",
    "num_workers = 0\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()                                      # Define the criterion (loss function). Here we will use Binary Cross Entropy with Logits\n",
    "optimizer = torch.optim.Adam(Feedforward(input_dim, hidden_size, output_dim).parameters(), lr=learning_rate)    # Define the optimizer with model parameters and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vizML model: \n",
    "parameters = {\n",
    "    'batch_size': 200,\n",
    "    'num_epochs': 100,\n",
    "    'hidden_sizes': [800, 800, 800],\n",
    "    'learning_rate': 5e-4,\n",
    "    'output_dim': len(chart_names),\n",
    "    'weight_decay': 0,\n",
    "    'dropout': 0.00,\n",
    "    'patience': 20,\n",
    "    'threshold': 1e-3,\n",
    "    'model_prefix': 'agg',\n",
    "    'save_model': False,\n",
    "    'print_test': True,\n",
    "    'test_best': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "This data is created from vizML's features data using the notebook \"Load and Clean Plotly Data\".\n",
    "Start there, or download features_with_9_chart_type_labels_888k.csv here (still waiting for it to upload, link coming soon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 888484 entries, 0 to 888483\n",
      "Columns: 849 entries, Unnamed: 0 to labels\n",
      "dtypes: bool(120), float64(726), int64(1), object(2)\n",
      "memory usage: 4.9+ GB\n"
     ]
    }
   ],
   "source": [
    "data_dir_name = '../data'\n",
    "data_file_name = 'features_with_chart_type_labels_888k.csv'\n",
    "df_full = pd.read_csv(os.path.join(data_dir_name, data_file_name))\n",
    "df_full.info()\n",
    "# df = df_full # uncomment this one to run everything on the full data set of 888k+ charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset Data\n",
    "Create a subset of the data for testing the notebook on a laptop. \n",
    "On a bigger machine, you can run the model on full data by uncommenting the df = df_full above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a random subset of data to be able to make and run models on my laptop\n",
    "# later, skip this step and run df=df_full to run model on full data set\n",
    "\n",
    "# create a series of weights corresponding to the frequency of each label in the data\n",
    "label_weights = df_full.groupby('labels')['labels'].transform('count')\n",
    "# create random sample \n",
    "subset_size = 10000\n",
    "RANDOM_STATE = 42\n",
    "df = df_full.sample(n=subset_size, replace=False, weights=label_weights, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bar</th>\n",
       "      <th>box</th>\n",
       "      <th>heatmap</th>\n",
       "      <th>histogram</th>\n",
       "      <th>line</th>\n",
       "      <th>pie</th>\n",
       "      <th>scatter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>338859</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845535</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654110</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537280</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143080</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bar  box  heatmap  histogram  line  pie  scatter\n",
       "338859    0    0        0          0     0    0        1\n",
       "845535    0    0        0          0     1    0        0\n",
       "654110    0    0        0          0     0    0        1\n",
       "537280    0    0        0          0     0    0        1\n",
       "143080    0    0        0          0     1    0        0"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make features data frame without ids or chart type labels\n",
    "# save a list of column labels for later\n",
    "chart_names = df['labels'].unique().tolist()\n",
    "\n",
    "# one-hot encode labels\n",
    "labels = pd.get_dummies(df['labels'])\n",
    "labels.head() # take a peek at one-hot encoded labels\n",
    "\n",
    "features = df.iloc[:,2:]\n",
    "features.drop('labels', axis=1, inplace=True)\n",
    "# we need to encode label names to a zero-indexed series of intigers\n",
    "# create a dictionary of classes to indexes\n",
    "## class2idx = {val : idx for idx, val in enumerate(chart_names)}\n",
    "# and a dictionary of the reverse to use later to reverse the encoding\n",
    "## idx2class = {v: k for k, v in class2idx.items()}\n",
    "# create (encoded) labels data frame\n",
    "## labels = df['labels'].replace(class2idx)\n",
    "## print(class2idx)\n",
    "\n",
    "labels.head() # take a peek at encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split train/val/test data and save matricies to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 7650, X_val: 1350, X_test: 1000, Total: 10000\n",
      "y_train: 7650, y_val: 1350, y_test: 1000, Total: 10000\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas dataframe data to numpy datatypes\n",
    "X = features.to_numpy()                     # X is the image ID (the first column in the dataframe)\n",
    "y = labels.to_numpy()   # y is the one-hot encoded label vector (all but the first column in the dataframe)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)               # Create train and test sets from entire dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)    # Create train and val sets from train set. Here is where you could also do k-fold cross validation.\n",
    "\n",
    "print(f'X_train: {len(X_train)}, X_val: {len(X_val)}, X_test: {len(X_test)}, Total: {len(X_train)+len(X_val)+len(X_test)}')\n",
    "print(f'y_train: {len(y_train)}, y_val: {len(y_val)}, y_test: {len(y_test)}, Total: {len(y_train)+len(y_val)+len(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast data as torch tensors\n",
    "X_train, y_train = torch.from_numpy(\n",
    "    X_train.astype(np.float64, copy=False)), torch.from_numpy(y_train.astype(np.float64))\n",
    "X_val, y_val = torch.from_numpy(X_val.astype(np.float64, copy=False)), torch.from_numpy(y_val)\n",
    "X_test, y_test = torch.from_numpy(X_test.astype(np.float64, copy=False)), torch.from_numpy(y_test)\n",
    "\n",
    "# load datasets\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "# create dataloaders that handle batch sizes\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, shuffle=True, batch_size=batch_size, num_workers=num_workers)\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, shuffle=True, batch_size=batch_size, num_workers=num_workers)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, shuffle=True, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Device and Define Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is  cpu   dtype is  <class 'torch.FloatTensor'>\n"
     ]
    }
   ],
   "source": [
    "# set device to cuda if available, cpu if not\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "print(\"device is \", device, \"  dtype is \", dtype)\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model and train, test, and fit options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a model that can be passed the above parameters\n",
    "class Feedforward(nn.Module):\n",
    "    '''\n",
    "    This is my own model, more explicitly written\n",
    "    Less flexible than AdvancedNet (from VizML, below) becayse you can't change the \n",
    "    architecture with a change to hidden_sizes -- but identical otherwise.\n",
    "    Useful for debugging the cuda stuff.\n",
    "    Change models below \n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_size, output_dim, dropout=0.0):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_dim = output_dim\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, output_dim)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.relu(self.fc3(x))\n",
    "        x = nn.functional.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=0)\n",
    "    \n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()                                                  # Set model parameters to training mode\n",
    "    running_loss = 0.0                                             # Keep track of running loss so average loss can be calculated at the end of the epoch\n",
    "    all_true = []                                                  # Keep track of all of the true and predicted labels\n",
    "    all_pred = []                                                  #   so AUC score can be calculated at the end of the epoch\n",
    "    all_scores = []\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(dataloader):              # Loop through all batches in the dataloader\n",
    "        \n",
    "        all_true += np.argmax(labels, axis=1).tolist()                                # Save true labels\n",
    "        inputs = inputs.to(device)                                 # Send inputs and labels to computing device. If the model is on 'cuda',\n",
    "        labels = labels.to(device)                                 #   the inputs and labels must also be on cuda\n",
    "        \n",
    "        optimizer.zero_grad()                                      # Zero the gradients so the optimizer can keep track of a new pass of data through the network\n",
    "        pred = model(inputs.to(dtype=torch.float, device=device))  # Pass a batch through the model\n",
    "        loss = criterion(pred, labels)        # Calculate the loss between the ground truth labels and the model predictions\n",
    "        loss.backward()                                            # Perform backpropagation on the loss to train the network\n",
    "        optimizer.step()                                           # Step the optimizer forward\n",
    "        \n",
    "        all_pred += np.argmax(pred.detach().numpy(), axis=1).tolist()                   # Keep track of model predictions to calculate AUC later.\n",
    "                                                                   # When we want to make predictions with our model without calculating BCEWithLogitsLoss,\n",
    "                                                                   #   we must manually apply the sigmoid function to the model output to get predicted values\n",
    "                                                                   #   between 0 and 1. tolist() simply converts the result to a normal Python list.\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    return running_loss / (i+1) , f1_score(all_true, all_pred, average='micro')  # Return the average loss over the epoch and the f1 micro score\n",
    "\n",
    "def test(model, dataloader, criterion, device):\n",
    "    model.eval()                                                   # Set model parameters to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        all_true += np.argmax(labels, axis=1).tolist()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device=device)\n",
    "        \n",
    "        with torch.no_grad():                                      # Don't track gradients through the model\n",
    "            model(inputs.to(dtype=torch.float, device=device)) \n",
    "            loss = criterion(pred, labels)\n",
    "            all_pred += np.argmax(pred.detach().numpy(), axis=1).tolist()\n",
    "\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    return running_loss / (i+1), f1_score(all_true, all_pred, average='micro') \n",
    "\n",
    "def fit(model, train_dataloader, val_dataloader, optimizer, criterion, device, num_epochs):\n",
    "    # Log start time\n",
    "    start_time = str(datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S'))\n",
    "    msg = f'Start model training {start_time}'\n",
    "    with open('log.txt', 'a') as f: f.write(msg+'\\n')\n",
    "    print(msg)\n",
    "    \n",
    "    # Initialize best model weights, AUC score, and keep track of train/val loss and AUC\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_model_name = ''\n",
    "    best_score = -1\n",
    "    track_values = {'train_loss': [],\n",
    "                    'val_loss': [],\n",
    "                    'train_score': [],\n",
    "                    'val_score': []}\n",
    "\n",
    "    # Calculate initial loss and score on train and validation sets\n",
    "    start = time.time()\n",
    "    train_loss, train_score = test(model, train_dataloader, criterion, device)\n",
    "    val_loss, val_score = test(model, val_dataloader, criterion, device)\n",
    "    \n",
    "    # Store initial losses and AUC scores\n",
    "    track_values['train_loss'].append(train_loss)\n",
    "    track_values['val_loss'].append(val_loss)\n",
    "    track_values['train_score'].append(train_score)\n",
    "    track_values['val_score'].append(val_score)\n",
    "    \n",
    "    # Print training status and write to a log file\n",
    "    msg = f'Epoch 0/{num_epochs} Train Loss: {train_loss:.4f}, Train AUC: {train_score:.4f}, Val Loss: {val_loss:.4f}, Val AUC: {val_score:.4f} Time: {time.time()-start:.2f}s'\n",
    "    with open('log.txt', 'a') as f: f.write(msg+'\\n')\n",
    "    print(msg)\n",
    "\n",
    "    # Loop over the specified number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time() # Start timer to keep track of how long an epoch takes\n",
    "\n",
    "        # Run train and test functions on train and val sets\n",
    "        train_loss, train_score = train(model, train_dataloader, optimizer, criterion, device)\n",
    "        val_loss, val_score = test(model, val_dataloader, criterion, device)\n",
    "        \n",
    "        # Store losses and AUC scores\n",
    "        track_values['train_loss'].append(train_loss)\n",
    "        track_values['val_loss'].append(val_loss)\n",
    "        track_values['train_auc'].append(train_score)\n",
    "        track_values['val_auc'].append(val_score)\n",
    "\n",
    "        # Save the model weights if the AUC score on the validation set is higher than the previous best model\n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if os.path.exists(best_model_name): os.remove(best_model_name)\n",
    "            best_model_name = f'./best_model_weights_epoch_{epoch+1}_auc_{val_score:.4f}.pt'\n",
    "            torch.save(model.state_dict(), best_model_name)\n",
    "\n",
    "        # Print training status and write to a log file\n",
    "        msg = f'Epoch {epoch+1}/{num_epochs} Train Loss: {train_loss:.4f}, Train AUC: {train_score:.4f}, Val Loss: {val_loss:.4f}, Val AUC: {val_score:.4f} Time: {time.time()-start:.2f}s'\n",
    "        with open('log.txt', 'a') as f: f.write(msg+'\\n')\n",
    "        print(msg)\n",
    "    \n",
    "    return track_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true = []                                                  # Keep track of all of the true and predicted labels\n",
    "all_pred = []                                                  #   so AUC score can be calculated at the end of the epoch\n",
    "all_scores = []\n",
    "    \n",
    "for i, (inputs, labels) in enumerate(train_dataloader):              # Loop through all batches in the dataloader\n",
    "        \n",
    "    all_true += np.argmax(labels, axis=1).tolist()                                # Save true labels\n",
    "    inputs = inputs.to(device)                                 # Send inputs and labels to computing device. If the model is on 'cuda',\n",
    "    labels = labels.to(device)                                 #   the inputs and labels must also be on cuda\n",
    "        \n",
    "    optimizer.zero_grad()                                      # Zero the gradients so the optimizer can keep track of a new pass of data through the network\n",
    "    pred = model(inputs.to(dtype=torch.float, device=device))  # Pass a batch through the model\n",
    "    loss = criterion(pred, labels)        # Calculate the loss between the ground truth labels and the model predictions\n",
    "    loss.backward()                                            # Perform backpropagation on the loss to train the network\n",
    "    optimizer.step()                                           # Step the optimizer forward\n",
    "        \n",
    "    all_pred += np.argmax(pred.detach().numpy(), axis=1).tolist()                   # Keep track of model predictions to calculate AUC later.\n",
    "                                                                   # When we want to make predictions with our model without calculating BCEWithLogitsLoss,\n",
    "                                                                   #   we must manually apply the sigmoid function to the model output to get predicted values\n",
    "                                                                   #   between 0 and 1. tolist() simply converts the result to a normal Python list. \n",
    "    running_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-360-6a9b42af1c0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'inputs'"
     ]
    }
   ],
   "source": [
    "for i, (inputs,lables) in enumerate(train_dataloader:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7571690674515563, 0.14143790849673202)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model, train_dataloader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model training 2020-07-01-11-49-23\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([200, 7])) must be the same as input size (torch.Size([50, 7]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-355-8e1e65a60a86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# start training based on my model and train/test/fit functions:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrack_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-351-5355cfb36d26>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, criterion, device, num_epochs)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Calculate initial loss and score on train and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-351-5355cfb36d26>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, dataloader, criterion, device)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                                      \u001b[0;31m# Don't track gradients through the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mall_pred\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                                                   reduction=self.reduction)\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   2122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2124\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([200, 7])) must be the same as input size (torch.Size([50, 7]))"
     ]
    }
   ],
   "source": [
    "# start training based on my model and train/test/fit functions: \n",
    "model = Feedforward(input_dim, hidden_size, output_dim)\n",
    "track_values = fit(model, train_dataloader, val_dataloader, optimizer, criterion, device, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate method: The VizML model, dataloaders, evaluations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of total examples is  10000\n",
      "indexes for splitting between train/val/test are  [8000, 9000]\n"
     ]
    }
   ],
   "source": [
    "# split 10% of examples into val, and 10% into test\n",
    "# save matrecies to disk as np arrays using custom function from vizML\n",
    "util.save_matrices_to_disk(\n",
    "    features, labels, [0.1, 0.1], saves_directory, parameters['model_prefix'], num_datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load those back from disk using another function from vizML\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = util.load_matrices_from_disk(\n",
    "        saves_directory, parameters['model_prefix'], num_datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedNet(nn.Module):\n",
    "    '''\n",
    "    I'm borrowing AdvancedNet from vizML. \n",
    "    It uses nn.ModuleList to construct a list of modules/layers according to the above paramiters. \n",
    "    hidden_sizes is a list where each intiger becomes the number of neurons in a hidden layer. \n",
    "    By default it is [800, 800, 800] so makes a network with 3 hidden layers.\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes, dropout=0.0):\n",
    "        super(AdvancedNet, self).__init__()\n",
    "        self.nn_list = nn.ModuleList()\n",
    "        self.nn_list.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        self.nn_list.append(nn.ReLU())\n",
    "        if dropout:\n",
    "            self.nn_list.append(nn.Dropout(p=dropout))\n",
    "\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            self.nn_list.append(\n",
    "                nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "            self.nn_list.append(nn.ReLU())\n",
    "            if dropout:\n",
    "                self.nn_list.append(nn.Dropout(p=dropout))\n",
    "        self.nn_list.append(nn.Linear(hidden_sizes[-1], num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self.nn_list:\n",
    "            x = x.float()\n",
    "            x = module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(X_train, y_train, X_val, y_val,\n",
    "                  parameters, X_test=None, y_test=None):\n",
    "\n",
    "    # calculate output dim\n",
    "    y_combined = np.concatenate((y_train, y_val))\n",
    "    if y_test is not None:\n",
    "        y_combined = np.concatenate((y_combined, y_test))\n",
    "    output_dim = len(np.unique(y_combined))\n",
    "    print('output_dim is', output_dim)\n",
    "    parameters['input_dim'] = X_train.shape[1]\n",
    "    parameters['output_dim'] = output_dim\n",
    "\n",
    "    # datasets\n",
    "    # convert np matrices into torch Variables, and then feed them into a\n",
    "    # dataloader\n",
    "    # photon note: changed to cast X as float64\n",
    "    X_train, y_train = torch.from_numpy(\n",
    "        X_train.astype(np.float64, copy=False)), torch.from_numpy(y_train.astype(np.float64))\n",
    "    X_val, y_val = torch.from_numpy(X_val.astype(np.float64, copy=False)), torch.from_numpy(y_val)\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n",
    "    test_dataloader = None\n",
    "\n",
    "    if X_test is not None:\n",
    "        X_test, y_test = torch.from_numpy(\n",
    "            X_test.astype(np.float64, copy=False)), torch.from_numpy(y_test)\n",
    "        test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "        test_dataloader = torch.utils.data.DataLoader(\n",
    "            test_dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dim is 2\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = load_datasets(\n",
    "    X_train.astype(np.float64), y_train.astype(np.float64), X_val.astype(np.float64), y_val.astype(np.float64), parameters, X_test=X_test.astype(np.float64), y_test=y_test.astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at 2020-07-01 00:33:10\n",
      "batch_size=200, dropout=0.0, hidden_sizes=[800, 800, 800], input_dim=846, learning_rate=0.0005, model_prefix='agg', num_epochs=100, output_dim=7, patience=20, print_test=True, save_model=False, test_best=False, threshold=0.001, weight_decay=0\n",
      "starting training\n",
      "epoch: 1, lr: 5.0e-04    2020-07-01 00:33:10\n",
      "train acc: 0.6425\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-62d862bfe54c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         val_accuracy, total_loss = evaluate.eval_error(\n\u001b[0;32m---> 92\u001b[0;31m             net, val_dataloader, criterion)\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val acc: %.4f, loss: %.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# remember: feed val loss into scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/viz/neural_network/evaluate.py\u001b[0m in \u001b[0;36meval_error\u001b[0;34m(net, loader, criterion)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m             raise RuntimeError(\n\u001b[1;32m    195\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# start training \n",
    "# using vizML's model and train functions\n",
    "\n",
    "# nets and optimizers\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "net = AdvancedNet(\n",
    "    input_dim,\n",
    "    hidden_sizes,\n",
    "    output_dim,\n",
    "    dropout=dropout).to(device)\n",
    "optimizer = optim.Adam(\n",
    "    net.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay)\n",
    "# ReduceLROnPlateau reduces learning rate by factor of 10 once val loss\n",
    "# has plateaued\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=patience, threshold=threshold)\n",
    "\n",
    "num_train_batches = len(train_dataloader)\n",
    "epoch = 1\n",
    "best_epoch, best_acc = 0, 0\n",
    "train_acc = [0]\n",
    "\n",
    "print('Starting training at ' + util.get_time())\n",
    "print(', '.join(['{}={!r}'.format(k, v)\n",
    "                for k, v in sorted(parameters.items())]))\n",
    "\n",
    "# print out test accuracies to a separate file\n",
    "suffix=''\n",
    "test_file = None\n",
    "if print_test:\n",
    "    test_file = open('test{}.txt'.format(suffix), 'a')\n",
    "    test_file.write('\\n\\n')\n",
    "    test_file.write('Starting at ' + util.get_time() + '\\n')\n",
    "    test_file.write(', '.join(['{}={!r}'.format(k, v) for k, v in sorted(parameters.items())]) + '\\n\\n')\n",
    "\n",
    "print('starting training')\n",
    "while epoch <= num_epochs:\n",
    "    running_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "\n",
    "    net.train()\n",
    "    print(\n",
    "        'epoch: %d, lr: %.1e' %\n",
    "        (epoch,\n",
    "        optimizer.param_groups[0]['lr']) +\n",
    "        '    ' +\n",
    "        util.get_time())\n",
    "    for batch_num, (inputs, labels) in enumerate(train_dataloader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = inputs.to(device), labels.to(dtype=torch.long, device=device) # changing labels dtype to torch.long seems like it worked\n",
    "        outputs = net(inputs) # here's where it was hanging up before\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # output is 2D array (logsoftmax output), so we flatten it to a 1D to get the max index for each example\n",
    "        # and then calculate accuracy off that\n",
    "        max_index = outputs.max(dim=1)[1]\n",
    "        epoch_acc += np.sum(max_index.data.cpu().numpy()\n",
    "                           == labels.data.cpu().numpy()) / inputs.size()[0]\n",
    "\n",
    "        # output every output_period batches\n",
    "        if output_period:\n",
    "            if batch_num % output_period == 0:\n",
    "                print('[%d:%.2f] loss: %.3f' % (\n",
    "                    epoch, batch_num * 1.0 / num_train_batches,\n",
    "                    running_loss / output_period))\n",
    "                running_loss = 0.0\n",
    "                gc.collect()\n",
    "\n",
    "    # save model after every epoch in models/ folder\n",
    "    if save_model:\n",
    "        torch.save(\n",
    "            net.state_dict(),\n",
    "            models_directory +\n",
    "            '/' +\n",
    "            model_prefix +\n",
    "            \".%d\" %\n",
    "            epoch)\n",
    "\n",
    "    # print training/val accuracy\n",
    "    epoch_acc = epoch_acc / num_train_batches\n",
    "    train_acc.append(epoch_acc)\n",
    "    print('train acc: %.4f' % (epoch_acc))\n",
    "    if only_train:\n",
    "        scheduler.step(loss)\n",
    "    else:\n",
    "        val_accuracy, total_loss = evaluate.eval_error(\n",
    "            net, val_dataloader, criterion)\n",
    "        print('val acc: %.4f, loss: %.4f' % (val_accuracy, total_loss))\n",
    "        # remember: feed val loss into scheduler\n",
    "        scheduler.step(total_loss)\n",
    "        if val_accuracy > best_acc:\n",
    "            best_epoch, best_acc = epoch, val_accuracy\n",
    "        print()\n",
    "\n",
    "        # write test accuracy\n",
    "        if print_test:\n",
    "            test_accuracy, total_loss = evaluate.eval_error(\n",
    "                net, test_dataloader, criterion)\n",
    "            test_file.write(\n",
    "                'epoch: %d' %\n",
    "                (epoch) +\n",
    "                '    ' +\n",
    "                util.get_time() +\n",
    "                '\\n')\n",
    "            test_file.write('train acc: %.4f' % (epoch_acc) + '\\n')\n",
    "            test_file.write('val acc: %.4f' % (val_accuracy) + '\\n')\n",
    "            test_file.write('test acc: %.4f' % (test_accuracy) + '\\n')\n",
    "            test_file.write('loss: %.4f' % (test_accuracy) + '\\n')\n",
    "\n",
    "        gc.collect()\n",
    "            \n",
    "    # perform early stopping here if our learning rate is below a threshold\n",
    "    # because small lr means little change in accuracy anyways\n",
    "    if optimizer.param_groups[0]['lr'] < (0.9 * 0.01 * learning_rate):\n",
    "        print('Low LR reached, finishing training early')\n",
    "        break\n",
    "    epoch += 1\n",
    "\n",
    "print('best epoch: %d' % best_epoch)\n",
    "print('best val accuracy: %.4f' % best_acc)\n",
    "print('train accuracy at that epoch: %.4f' % train_acc[best_epoch])\n",
    "print('ending at', time.ctime())\n",
    "\n",
    "if test_best:\n",
    "    net.load_state_dict(\n",
    "        torch.load(\n",
    "            models_directory +\n",
    "            '/' +\n",
    "            model_prefix +\n",
    "            '.' +\n",
    "            str(best_epoch)))\n",
    "    best_test_accuracy, total_loss = evaluate.eval_error(\n",
    "        net, test_dataloader, criterion)\n",
    "    test_file.write('*****\\n')\n",
    "    test_file.write(\n",
    "        'best test acc: %.4f, loss: %.4f' %\n",
    "        (best_test_accuracy, total_loss) + '\\n')\n",
    "    test_file.write('*****\\n')\n",
    "    print('best test acc: %.4f, loss: %.4f' %\n",
    "        (best_test_accuracy, total_loss))\n",
    "\n",
    "if print_test:\n",
    "    test_file.write('\\n')\n",
    "    test_file.close()\n",
    "\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdvancedNet(\n",
       "  (nn_list): ModuleList(\n",
       "    (0): Linear(in_features=846, out_features=800, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=800, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdvancedNet(input_dim, hidden_sizes, output_dim).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
