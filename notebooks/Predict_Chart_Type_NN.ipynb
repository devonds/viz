{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Chart Type with a Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "base_path = os.path.abspath(os.path.join('..'))\n",
    "if base_path not in sys.path:\n",
    "    sys.path.append(base_path)\n",
    "    \n",
    "import neural_network.util as util\n",
    "import neural_network.train as train\n",
    "import neural_network.evaluate as evaluate\n",
    "from helpers.processing import *\n",
    "from helpers.analysis import *\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "features_directory = '../features/processed'\n",
    "saves_directory = '../neural_network/saves'\n",
    "num_datapoints = None  # None if you want all of the data\n",
    "model_prefix = 'agg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the parameters can be adjusted here\n",
    "\n",
    "# num_epochs: the max number of epochs we will train the NN for\n",
    "# hidden_sizes: the number of neurons in each hidden layer, enter it as a list\n",
    "# output_dim: the dimension of the output. Since outputs are 'line', 'scatter', 'bar', it's 9\n",
    "#                                                            + 'circos' ; +  'table' will be 5\n",
    "# weight_decay: how much to decay LR in the NN. This can be set to 0 since we decrease LR already through\n",
    "#   the ReduceLROnPlateau() function\n",
    "# dropout: the dropout in each layer\n",
    "# patience: how many epochs we go through (with a near constant learning rate, this threshold is adjusted using\n",
    "#   threshold) before dropping learning rate by a factor of 10\n",
    "# model_prefix: all models will be loaded/saved with the prefix of the file in the beginning\n",
    "# save_model: save each epoch's model onto models/ folder.\n",
    "# print_test: print test accuracies into test.txt\n",
    "# test_best: test the test accuracy of the best model we've found (best\n",
    "# model determined using val accuracy)\n",
    "\n",
    "# note: training is automatically stopped when learning rate < 0.01 *\n",
    "# starting learning rate\n",
    "\n",
    "parameters = {\n",
    "    'batch_size': 200,\n",
    "    'num_epochs': 100,\n",
    "    'hidden_sizes': [800, 800, 800],\n",
    "    'learning_rate': 5e-4,\n",
    "    'output_dim': len(chart_names),\n",
    "    'weight_decay': 0,\n",
    "    'dropout': 0.00,\n",
    "    'patience': 20,\n",
    "    'threshold': 1e-3,\n",
    "    'model_prefix': 'agg',\n",
    "    'save_model': False,\n",
    "    'print_test': True,\n",
    "    'test_best': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "This data is created from vizML's features data using the notebook \"Load and Clean Plotly Data\".\n",
    "Start there, or download features_with_9_chart_type_labels_888k.csv here (still waiting for it to upload, link coming soon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 888484 entries, 0 to 888483\n",
      "Columns: 849 entries, Unnamed: 0 to labels\n",
      "dtypes: bool(120), float64(726), int64(1), object(2)\n",
      "memory usage: 4.9+ GB\n"
     ]
    }
   ],
   "source": [
    "data_dir_name = '../data'\n",
    "data_file_name = 'features_with_chart_type_labels_888k.csv'\n",
    "df_full = pd.read_csv(os.path.join(data_dir_name, data_file_name))\n",
    "df_full.info()\n",
    "# df = df_full # uncomment this one to run everything on the full data set of 888k+ charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset Data\n",
    "Create a subset of the data for testing the notebook on a laptop. \n",
    "On a bigger machine, you can run the model on full data by uncommenting the df = df_full above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a random subset of data to be able to make and run models on my laptop\n",
    "# later, skip this step and run df=df_full to run model on full data set\n",
    "\n",
    "# create a series of weights corresponding to the frequency of each label in the data\n",
    "label_weights = df_full.groupby('labels')['labels'].transform('count')\n",
    "# create random sample \n",
    "subset_size = 10000\n",
    "RANDOM_STATE = 42\n",
    "df = df_full.sample(n=subset_size, replace=False, weights=label_weights, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scatter': 0, 'line': 1, 'box': 2, 'bar': 3, 'histogram': 4, 'pie': 5, 'heatmap': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "338859    0\n",
       "845535    1\n",
       "654110    0\n",
       "537280    0\n",
       "143080    1\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make features data frame without ids or chart type labels\n",
    "# save a list of column labels for later\n",
    "chart_names = df['labels'].unique().tolist()\n",
    "features = df.iloc[:,2:]\n",
    "features.drop('labels', axis=1, inplace=True)\n",
    "# we need to encode label names to a zero-indexed series of intigers\n",
    "# create a dictionary of classes to indexes\n",
    "class2idx = {val : idx for idx, val in enumerate(chart_names)}\n",
    "# and a dictionary of the reverse to use later to reverse the encoding\n",
    "idx2class = {v: k for k, v in class2idx.items()}\n",
    "# create (encoded) labels data frame\n",
    "labels = df['labels'].replace(class2idx)\n",
    "print(class2idx)\n",
    "labels.head() # take a peek at encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split train/val/test data and save matricies to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of total examples is  10000\n",
      "indexes for splitting between train/val/test are  [8000, 9000]\n"
     ]
    }
   ],
   "source": [
    "# split 10% of examples into val, and 10% into test\n",
    "# save matrecies to disk as np arrays using custom function from vizML\n",
    "util.save_matrices_to_disk(\n",
    "    features, labels, [0.1, 0.1], saves_directory, parameters['model_prefix'], num_datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load those back from disk using another function from vizML\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = util.load_matrices_from_disk(\n",
    "        saves_directory, parameters['model_prefix'], num_datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(X_train, y_train, X_val, y_val,\n",
    "                  parameters, X_test=None, y_test=None):\n",
    "\n",
    "    # calculate output dim\n",
    "    y_combined = np.concatenate((y_train, y_val))\n",
    "    if y_test is not None:\n",
    "        y_combined = np.concatenate((y_combined, y_test))\n",
    "    output_dim = len(np.unique(y_combined))\n",
    "    print('output_dim is', output_dim)\n",
    "    parameters['input_dim'] = X_train.shape[1]\n",
    "    parameters['output_dim'] = output_dim\n",
    "\n",
    "    # datasets\n",
    "    # convert np matrices into torch Variables, and then feed them into a\n",
    "    # dataloader\n",
    "    # photon note: changed to cast X as float64\n",
    "    X_train, y_train = torch.from_numpy(\n",
    "        X_train.astype(np.float64, copy=False)), torch.from_numpy(y_train.astype(np.float64))\n",
    "    X_val, y_val = torch.from_numpy(X_val.astype(np.float64, copy=False)), torch.from_numpy(y_val)\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n",
    "    test_dataloader = None\n",
    "\n",
    "    if X_test is not None:\n",
    "        X_test, y_test = torch.from_numpy(\n",
    "            X_test.astype(np.float64, copy=False)), torch.from_numpy(y_test)\n",
    "        test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "        test_dataloader = torch.utils.data.DataLoader(\n",
    "            test_dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dim is 7\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = load_datasets(\n",
    "    X_train.astype(np.float64), y_train.astype(np.float64), X_val.astype(np.float64), y_val.astype(np.float64), parameters, X_test=X_test.astype(np.float64), y_test=y_test.astype(np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Device and Define Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is  cpu   dtype is  <class 'torch.FloatTensor'>\n"
     ]
    }
   ],
   "source": [
    "# set device to cuda if available, cpu if not\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "print(\"device is \", device, \"  dtype is \", dtype)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the parameters can be adjusted here\n",
    "\n",
    "# num_epochs: the max number of epochs we will train the NN for\n",
    "# hidden_sizes: the number of neurons in each hidden layer, enter it as a list\n",
    "# output_dim: the dimension of the output. Since outputs are 'line', 'scatter', 'bar', it's 9\n",
    "#                                                            + 'circos' ; +  'table' will be 5\n",
    "# weight_decay: how much to decay LR in the NN. This can be set to 0 since we decrease LR already through\n",
    "#   the ReduceLROnPlateau() function\n",
    "# dropout: the dropout in each layer\n",
    "# patience: how many epochs we go through (with a near constant learning rate, this threshold is adjusted using\n",
    "#   threshold) before dropping learning rate by a factor of 10\n",
    "# model_prefix: all models will be loaded/saved with the prefix of the file in the beginning\n",
    "# save_model: save each epoch's model onto models/ folder.\n",
    "# print_test: print test accuracies into test.txt\n",
    "# test_best: test the test accuracy of the best model we've found (best model determined using val accuracy)\n",
    "\n",
    "# note: training is automatically stopped when learning rate < 0.01 *\n",
    "# batch_size is determined in the dataloader, so the variable is irrelevant here\n",
    "batch_size = 200\n",
    "num_epochs = 100\n",
    "hidden_sizes = [800, 800, 800] # AdvancedNet (from vizML) uses a list of hidden size values\n",
    "hidden_size = 800 # FeedForward model uses a single hidden size value\n",
    "learning_rate = 5e-4\n",
    "weight_decay = 0\n",
    "dropout = 0.0\n",
    "patience = 10\n",
    "threshold = 1e-3\n",
    "input_dim = X_train.shape[1]\n",
    "input_size = X_train.shape[1]\n",
    "output_dim = len(chart_names)\n",
    "output_period = 0 # output_period: output training loss every x batches\n",
    "model_prefix = 'agg'\n",
    "only_train = False\n",
    "save_model = False\n",
    "test_best = False\n",
    "print_test = True\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()                                      # Define the criterion (loss function). Here we will use Binary Cross Entropy with Logits\n",
    "optimizer = torch.optim.Adam(Feedforward(input_dim, hidden_size, output_dim).parameters(), lr=learning_rate)    # Define the optimizer with model parameters and learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model and train, test, and fit options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a model that can be passed the above parameters\n",
    "class AdvancedNet(nn.Module):\n",
    "    '''\n",
    "    I'm borrowing AdvancedNet from vizML. \n",
    "    It uses nn.ModuleList to construct a list of modules/layers according to the above paramiters. \n",
    "    hidden_sizes is a list where each intiger becomes the number of neurons in a hidden layer. \n",
    "    By default it is [800, 800, 800] so makes a network with 3 hidden layers.\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes, dropout=0.0):\n",
    "        super(AdvancedNet, self).__init__()\n",
    "        self.nn_list = nn.ModuleList()\n",
    "        self.nn_list.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        self.nn_list.append(nn.ReLU())\n",
    "        if dropout:\n",
    "            self.nn_list.append(nn.Dropout(p=dropout))\n",
    "\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            self.nn_list.append(\n",
    "                nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "            self.nn_list.append(nn.ReLU())\n",
    "            if dropout:\n",
    "                self.nn_list.append(nn.Dropout(p=dropout))\n",
    "        self.nn_list.append(nn.Linear(hidden_sizes[-1], num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self.nn_list:\n",
    "            x = x.float()\n",
    "            x = module(x)\n",
    "        return x\n",
    "    \n",
    "class Feedforward(nn.Module):\n",
    "    '''\n",
    "    This is my own model, more explicitly written\n",
    "    Less flexible than AdvancedNet becayse you can't change the \n",
    "    architecture with a change to hidden_sizes -- but identical otherwise.\n",
    "    Useful for debugging the cuda stuff.\n",
    "    Change models below \n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_size, output_dim, dropout=0.0):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_dim = output_dim\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, output_dim)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.relu(self.fc3(x))\n",
    "        x = nn.functional.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=0)\n",
    "    \n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()                                                  # Set model parameters to training mode\n",
    "    running_loss = 0.0                                             # Keep track of running loss so average loss can be calculated at the end of the epoch\n",
    "    all_true = []                                                  # Keep track of all of the true and predicted labels\n",
    "    all_pred = []                                                  #   so AUC score can be calculated at the end of the epoch\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(dataloader):              # Loop through all batches in the dataloader\n",
    "        \n",
    "        all_true += labels.tolist()                                # Save true labels\n",
    "        inputs = inputs.to(device)                                 # Send images and labels to computing device. If the model is on 'cuda',\n",
    "        labels = labels.to(device)                                 #   the images and labels must also be on cuda\n",
    "        \n",
    "        optimizer.zero_grad()                                      # Zero the gradients so the optimizer can keep track of a new pass of data through the network\n",
    "        pred = model(inputs.to(dtype=torch.float, device=device))  # Pass a batch through the model\n",
    "        loss = criterion(pred, labels.to(dtype=torch.long))        # Calculate the loss between the ground truth labels and the model predictions\n",
    "        loss.backward()                                            # Perform backpropagation on the loss to train the network\n",
    "        optimizer.step()                                           # Step the optimizer forward\n",
    "        \n",
    "        all_pred += torch.sigmoid(pred).tolist()                   # Keep track of model predictions to calculate AUC later.\n",
    "                                                                   # When we want to make predictions with our model without calculating BCEWithLogitsLoss,\n",
    "                                                                   #   we must manually apply the sigmoid function to the model output to get predicted values\n",
    "                                                                   #   between 0 and 1. tolist() simply converts the result to a normal Python list.\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        scores = torch.sum((np.array(all_true) - np.array(all_pred)) ** 2, dim=tuple(range(1, outputs.dim())))\n",
    "    return running_loss / (i+1), roc_auc_score(all_true, scores, average='weighted', multi_class='ovr')    # Return the average loss over the epoch and the AUC score\n",
    "\n",
    "def test(model, dataloader, criterion, device):\n",
    "    model.eval()                                                   # Set model parameters to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        all_true += labels.tolist()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(dtype=torch.long, device=device)\n",
    "        \n",
    "        with torch.no_grad():                                      # Don't track gradients through the model\n",
    "            model(inputs.to(dtype=torch.float, device=device)) \n",
    "            loss = criterion(pred, labels)\n",
    "            all_pred += torch.sigmoid(pred).tolist()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        scores = torch.sum((np.array(all_true) - np.array(all_pred)) ** 2, dim=tuple(range(1, outputs.dim())))\n",
    "    return running_loss / (i+1), roc_auc_score(all_true, scores, average='weighted', multi_class='ovr')\n",
    "\n",
    "def fit(model, train_dataloader, val_dataloader, optimizer, criterion, device, num_epochs):\n",
    "    # Log start time\n",
    "    start_time = str(datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S'))\n",
    "    msg = f'Start model training {start_time}'\n",
    "    with open('log.txt', 'a') as f: f.write(msg+'\\n')\n",
    "    print(msg)\n",
    "    \n",
    "    # Initialize best model weights, AUC score, and keep track of train/val loss and AUC\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_model_name = ''\n",
    "    best_score = -1\n",
    "    track_values = {'train_loss': [],\n",
    "                    'val_loss': [],\n",
    "                    'train_auc': [],\n",
    "                    'val_auc': []}\n",
    "\n",
    "    # Calculate initial loss and score on train and validation sets\n",
    "    start = time.time()\n",
    "    train_loss, train_score = test(model, train_dataloader, criterion, device)\n",
    "    val_loss, val_score = test(model, val_dataloader, criterion, device)\n",
    "    \n",
    "    # Store initial losses and AUC scores\n",
    "    track_values['train_loss'].append(train_loss)\n",
    "    track_values['val_loss'].append(val_loss)\n",
    "    track_values['train_auc'].append(train_score)\n",
    "    track_values['val_auc'].append(val_score)\n",
    "    \n",
    "    # Print training status and write to a log file\n",
    "    msg = f'Epoch 0/{num_epochs} Train Loss: {train_loss:.4f}, Train AUC: {train_score:.4f}, Val Loss: {val_loss:.4f}, Val AUC: {val_score:.4f} Time: {time.time()-start:.2f}s'\n",
    "    with open('log.txt', 'a') as f: f.write(msg+'\\n')\n",
    "    print(msg)\n",
    "\n",
    "    # Loop over the specified number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time() # Start timer to keep track of how long an epoch takes\n",
    "\n",
    "        # Run train and test functions on train and val sets\n",
    "        train_loss, train_score = train(model, train_dataloader, optimizer, criterion, device)\n",
    "        val_loss, val_score = test(model, val_dataloader, criterion, device)\n",
    "        \n",
    "        # Store losses and AUC scores\n",
    "        track_values['train_loss'].append(train_loss)\n",
    "        track_values['val_loss'].append(val_loss)\n",
    "        track_values['train_auc'].append(train_score)\n",
    "        track_values['val_auc'].append(val_score)\n",
    "\n",
    "        # Save the model weights if the AUC score on the validation set is higher than the previous best model\n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if os.path.exists(best_model_name): os.remove(best_model_name)\n",
    "            best_model_name = f'./best_model_weights_epoch_{epoch+1}_auc_{val_score:.4f}.pt'\n",
    "            torch.save(model.state_dict(), best_model_name)\n",
    "\n",
    "        # Print training status and write to a log file\n",
    "        msg = f'Epoch {epoch+1}/{num_epochs} Train Loss: {train_loss:.4f}, Train AUC: {train_score:.4f}, Val Loss: {val_loss:.4f}, Val AUC: {val_score:.4f} Time: {time.time()-start:.2f}s'\n",
    "        with open('log.txt', 'a') as f: f.write(msg+'\\n')\n",
    "        print(msg)\n",
    "    \n",
    "    return track_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model(inputs.to(dtype=torch.float, device=device))\n",
    "pred.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-191-8c22363f23de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'all_pred' is not defined"
     ]
    }
   ],
   "source": [
    "all_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (200,) (7,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-190-78ba5e9aded0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-189-dda5dcaf13c6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ovr'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Return the average loss over the epoch and the AUC score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (200,) (7,) "
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, optimizer, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model training 2020-07-01-00-33-25\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-8e1e65a60a86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# start training based on my model and train/test/fit functions:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrack_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-181-ec2aaf6574b3>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, criterion, device, num_epochs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;31m# Calculate initial loss and score on train and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-181-ec2aaf6574b3>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, dataloader, criterion, device)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                                      \u001b[0;31m# Don't track gradients through the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m###############\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mall_pred\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2019\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Double"
     ]
    }
   ],
   "source": [
    "# start training based on my model and train/test/fit functions: \n",
    "model = Feedforward(input_dim, hidden_size, output_dim)\n",
    "track_values = fit(model, train_dataloader, val_dataloader, optimizer, criterion, device, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at 2020-07-01 00:33:10\n",
      "batch_size=200, dropout=0.0, hidden_sizes=[800, 800, 800], input_dim=846, learning_rate=0.0005, model_prefix='agg', num_epochs=100, output_dim=7, patience=20, print_test=True, save_model=False, test_best=False, threshold=0.001, weight_decay=0\n",
      "starting training\n",
      "epoch: 1, lr: 5.0e-04    2020-07-01 00:33:10\n",
      "train acc: 0.6425\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-62d862bfe54c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         val_accuracy, total_loss = evaluate.eval_error(\n\u001b[0;32m---> 92\u001b[0;31m             net, val_dataloader, criterion)\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val acc: %.4f, loss: %.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# remember: feed val loss into scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/viz/neural_network/evaluate.py\u001b[0m in \u001b[0;36meval_error\u001b[0;34m(net, loader, criterion)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m             raise RuntimeError(\n\u001b[1;32m    195\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# start training \n",
    "# using vizML's model and train functions\n",
    "\n",
    "# nets and optimizers\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "net = AdvancedNet(\n",
    "    input_dim,\n",
    "    hidden_sizes,\n",
    "    output_dim,\n",
    "    dropout=dropout).to(device)\n",
    "optimizer = optim.Adam(\n",
    "    net.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay)\n",
    "# ReduceLROnPlateau reduces learning rate by factor of 10 once val loss\n",
    "# has plateaued\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=patience, threshold=threshold)\n",
    "\n",
    "num_train_batches = len(train_dataloader)\n",
    "epoch = 1\n",
    "best_epoch, best_acc = 0, 0\n",
    "train_acc = [0]\n",
    "\n",
    "print('Starting training at ' + util.get_time())\n",
    "print(', '.join(['{}={!r}'.format(k, v)\n",
    "                for k, v in sorted(parameters.items())]))\n",
    "\n",
    "# print out test accuracies to a separate file\n",
    "suffix=''\n",
    "test_file = None\n",
    "if print_test:\n",
    "    test_file = open('test{}.txt'.format(suffix), 'a')\n",
    "    test_file.write('\\n\\n')\n",
    "    test_file.write('Starting at ' + util.get_time() + '\\n')\n",
    "    test_file.write(', '.join(['{}={!r}'.format(k, v) for k, v in sorted(parameters.items())]) + '\\n\\n')\n",
    "\n",
    "print('starting training')\n",
    "while epoch <= num_epochs:\n",
    "    running_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "\n",
    "    net.train()\n",
    "    print(\n",
    "        'epoch: %d, lr: %.1e' %\n",
    "        (epoch,\n",
    "        optimizer.param_groups[0]['lr']) +\n",
    "        '    ' +\n",
    "        util.get_time())\n",
    "    for batch_num, (inputs, labels) in enumerate(train_dataloader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = inputs.to(device), labels.to(dtype=torch.long, device=device) # changing labels dtype to torch.long seems like it worked\n",
    "        outputs = net(inputs) # here's where it was hanging up before\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # output is 2D array (logsoftmax output), so we flatten it to a 1D to get the max index for each example\n",
    "        # and then calculate accuracy off that\n",
    "        max_index = outputs.max(dim=1)[1]\n",
    "        epoch_acc += np.sum(max_index.data.cpu().numpy()\n",
    "                           == labels.data.cpu().numpy()) / inputs.size()[0]\n",
    "\n",
    "        # output every output_period batches\n",
    "        if output_period:\n",
    "            if batch_num % output_period == 0:\n",
    "                print('[%d:%.2f] loss: %.3f' % (\n",
    "                    epoch, batch_num * 1.0 / num_train_batches,\n",
    "                    running_loss / output_period))\n",
    "                running_loss = 0.0\n",
    "                gc.collect()\n",
    "\n",
    "    # save model after every epoch in models/ folder\n",
    "    if save_model:\n",
    "        torch.save(\n",
    "            net.state_dict(),\n",
    "            models_directory +\n",
    "            '/' +\n",
    "            model_prefix +\n",
    "            \".%d\" %\n",
    "            epoch)\n",
    "\n",
    "    # print training/val accuracy\n",
    "    epoch_acc = epoch_acc / num_train_batches\n",
    "    train_acc.append(epoch_acc)\n",
    "    print('train acc: %.4f' % (epoch_acc))\n",
    "    if only_train:\n",
    "        scheduler.step(loss)\n",
    "    else:\n",
    "        val_accuracy, total_loss = evaluate.eval_error(\n",
    "            net, val_dataloader, criterion)\n",
    "        print('val acc: %.4f, loss: %.4f' % (val_accuracy, total_loss))\n",
    "        # remember: feed val loss into scheduler\n",
    "        scheduler.step(total_loss)\n",
    "        if val_accuracy > best_acc:\n",
    "            best_epoch, best_acc = epoch, val_accuracy\n",
    "        print()\n",
    "\n",
    "        # write test accuracy\n",
    "        if print_test:\n",
    "            test_accuracy, total_loss = evaluate.eval_error(\n",
    "                net, test_dataloader, criterion)\n",
    "            test_file.write(\n",
    "                'epoch: %d' %\n",
    "                (epoch) +\n",
    "                '    ' +\n",
    "                util.get_time() +\n",
    "                '\\n')\n",
    "            test_file.write('train acc: %.4f' % (epoch_acc) + '\\n')\n",
    "            test_file.write('val acc: %.4f' % (val_accuracy) + '\\n')\n",
    "            test_file.write('test acc: %.4f' % (test_accuracy) + '\\n')\n",
    "            test_file.write('loss: %.4f' % (test_accuracy) + '\\n')\n",
    "\n",
    "        gc.collect()\n",
    "            \n",
    "    # perform early stopping here if our learning rate is below a threshold\n",
    "    # because small lr means little change in accuracy anyways\n",
    "    if optimizer.param_groups[0]['lr'] < (0.9 * 0.01 * learning_rate):\n",
    "        print('Low LR reached, finishing training early')\n",
    "        break\n",
    "    epoch += 1\n",
    "\n",
    "print('best epoch: %d' % best_epoch)\n",
    "print('best val accuracy: %.4f' % best_acc)\n",
    "print('train accuracy at that epoch: %.4f' % train_acc[best_epoch])\n",
    "print('ending at', time.ctime())\n",
    "\n",
    "if test_best:\n",
    "    net.load_state_dict(\n",
    "        torch.load(\n",
    "            models_directory +\n",
    "            '/' +\n",
    "            model_prefix +\n",
    "            '.' +\n",
    "            str(best_epoch)))\n",
    "    best_test_accuracy, total_loss = evaluate.eval_error(\n",
    "        net, test_dataloader, criterion)\n",
    "    test_file.write('*****\\n')\n",
    "    test_file.write(\n",
    "        'best test acc: %.4f, loss: %.4f' %\n",
    "        (best_test_accuracy, total_loss) + '\\n')\n",
    "    test_file.write('*****\\n')\n",
    "    print('best test acc: %.4f, loss: %.4f' %\n",
    "        (best_test_accuracy, total_loss))\n",
    "\n",
    "if print_test:\n",
    "    test_file.write('\\n')\n",
    "    test_file.close()\n",
    "\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdvancedNet(\n",
       "  (nn_list): ModuleList(\n",
       "    (0): Linear(in_features=846, out_features=800, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=800, out_features=800, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=800, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdvancedNet(input_dim, hidden_sizes, output_dim).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
